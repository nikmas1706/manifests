## Основной манифест
apiVersion: apps/v1
kind: Deployment                                                     ## Используем deployment
metadata:
  name: mindbox
spec:
  replicas: 4                                                        ## При старте утилизируем больше ресурсов, пусть стартанут все 4 реплики (максимальное заявленое кол-во), далее, hpa уменьшит кол-во, при необходимости, на основе потребления цпу
  selector:
    matchLabels:
      app: mindbox
  template:
    metadata:
      labels:
        app: mindbox
    spec:
      containers:
      - name: go-k8s
        image: nexus-registry.mindbox.ru/service/mindbox:v1.0        ## Не используем latest на проде
        command:
          - ./mindbox                                                ## Запуск приложения из бинаря (пусть будет golang)

        readinessProbe:                                              ## Проверяем что сервис готов принимать трафик
          httpGet:  
            path: /health                                            ## Проверяем "ручку" нагего сервиса
            port: 8080    
          initialDelaySeconds: 10                                    ## Приложению требуется около 10 сек на инициализацию
          periodSeconds: 5         
        livenessProbe:                                               ## Так же будем периодически проверять доступность нашего сервиса по этой же http-ручке
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15                                    ## Через 15 секунд начнем делать проверки
          periodSeconds: 10                                          ## Будем проверять доступность каждые 10 сек.
        terminationGracePeriodSeconds: 20                            ## Значение из головы т.к. нет данных как приложение обрабатывает сигнал завершения, до 20 секунд должно хватить на "нормальное" завершение работы

        resources:
          requests:
            cpu: 150m                                                ## Из тз не понятно, сколько цпу нам надо в пике + на старте потреблние выше (тут за счет старта 4 реплик + hpa можно попробовать обойтись), 4 пода в пике, согласно тз, ни о чем не говорят т.к. нужны метрики потребления цпу, если было нагрузочное тестирование, то метрики можно взять оттуда и более правильно указать их. Если нам не доступны результаты нагрузочного, можно попробовать отдельно завести VPA (т.к. его лучше не использовать с HPA, при том что мы ориентируемся только на cpu) c модом "off", оценить потребление и указать значения реквестов и лимитов. 
            memory: 128Mi                                            ## Исходя из тз, потребление памяти у нас 128Mi, поэтому выставляем равные(гарантированные) реквесты и лимиты 
          limits:
            cpu: 400m                                                ## Тот же ответ, что и про реквесты выше, пусть будет 0.4 (под пик и возможные погрешности) + hpa на старте (4 реплики) т.е. 1.6 ядра в пике днем (и при старте). Хотя для старта можно еще попробовать убавить лимиты, для того чтобы не аллоцировать на ноде лишние ресурсы и добавить кол-во реплик, скажем, до 8 (далее hpa уже оставит нужное кол-во реплик в завимости от настроек, в нашем случае 2-4) 
            memory: 128Mi                                            ## Исходя из тз, потребление памяти у нас 128Mi, поэтому выставляем равные(гарантированные) реквесты и лимиты 

      restartPolicy: Always                                          ## Всегда перезапускаем приложение при сбое
      affinity:                                                      ## Для более гибкой настойки распределения подов можно так же использовать pod topology spread constraints
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - mindbox
                topologyKey: topology.kubernetes.io/zone              ## Указываем анти-аффинити с preferred для лейбла сервиса т.к. у нас в пике 4 пода, а зон 3 (иначе 4 реплика будет висеть в пендинге)
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - mindbox
                topologyKey: "kubernetes.io/hostname"                   ## Если у нас 5 нод, а зон 3 и по рузультатам нагрузочного тестирования нам нужно 4 реплики - значит что минимум в одной зоне будет 2 пода одного сервиса, если в этой зоне есть еще ноды, пусть под заедет на вторую ноду, для дополнительной отказоучтойчивости (тоже с preferred чтобы в случае чего не было пендинга)

  strategy:
    type: RollingUpdate                                               ## Будем обновляться "плавно", что добавит еще отказоустойчивости 
    rollingUpdate:
      maxUnavailable: 25%                                             ## Лучше указывать в процентах (если будет 4 реплики, то за раз обновляться будет одна)
      maxSurge: 25%
